{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import Normalize\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.models.base_unet import BaseUNet\n",
    "from src.models.unet_pp import UNetPlus\n",
    "from src.models.hr_spin import HRSPIN\n",
    "from src.utils.visualizations import plot_predictions\n",
    "from src.utils.io import load_image, save_mask\n",
    "from src.scripts.mask_to_submission import masks_to_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = os.listdir(\"../logs\")\n",
    "\n",
    "scores = []\n",
    "for dir in models:\n",
    "    metrics = json.load(open(f\"../logs/{dir}/metrics.json\", \"r\"))\n",
    "    args = argparse.Namespace(**json.load(open(f\"../logs/{dir}/config.json\", \"r\")))\n",
    "    scores.append((dir, args.model, max(metrics['val_acc'])))\n",
    "\n",
    "threshold = 0.925\n",
    "count = np.sum([1 for score in scores if score[-1] > threshold])\n",
    "print(f\"Number of models with score > {threshold}: {count}\")\n",
    "print()\n",
    "\n",
    "scores.sort(key=lambda x: x[-1], reverse=True)\n",
    "for score in scores:\n",
    "    print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "\n",
    "checkpoints = [f\"../logs/{score[0]}\" for score in scores[:N]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = \"../metadata.json\"\n",
    "\n",
    "metadata = json.load(open(metadata, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint):\n",
    "    with open(os.path.join(checkpoint, \"config.json\"), \"r\") as f:\n",
    "        vars = json.load(f)\n",
    "    args = argparse.Namespace(**vars)\n",
    "\n",
    "    chs = [3] + [2 ** (i + 5) for i in range(args.depth)]\n",
    "    if args.model == \"unet\":\n",
    "        model = BaseUNet(chs)\n",
    "    elif args.model == \"unet++\":\n",
    "        model = UNetPlus(chs)\n",
    "    elif args.model == \"spin\":\n",
    "        model = HRSPIN(num_stacks=args.num_stacks)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name\")\n",
    "    \n",
    "    model.load_state_dict(torch.load(os.path.join(checkpoint, \"best_model.pt\"), map_location=torch.device(\"cpu\")))\n",
    "\n",
    "    return model, args"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = \"../data/test/images/\"\n",
    "\n",
    "fnames = os.listdir(test_images)\n",
    "fnames = [os.path.join(test_images, fname) for fname in fnames if fname.endswith(\".png\")]\n",
    "\n",
    "len(fnames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask(path, checkpoints):\n",
    "    mean = metadata[\"cil\"][\"img_mean\"]\n",
    "    std = metadata[\"cil\"][\"img_std\"]\n",
    "\n",
    "    transform = Normalize(mean=mean, std=std)\n",
    "\n",
    "    image = load_image(path)\n",
    "    image = torch.tensor(image)\n",
    "    image = image.permute(2, 0, 1).unsqueeze(0).float()\n",
    "    image = transform(image)\n",
    "\n",
    "    preds = []\n",
    "    for ckpt in checkpoints:\n",
    "        model, args = load_model(ckpt)\n",
    "\n",
    "        model.eval()\n",
    "        model.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            if args.model == \"spin\":\n",
    "                prediction, _ = model(image.to(DEVICE))\n",
    "                prediction = prediction[-1].squeeze(0)\n",
    "            else:\n",
    "                prediction = model(image.to(DEVICE)).squeeze(0)\n",
    "\n",
    "            preds.append(prediction.cpu())\n",
    "\n",
    "    return torch.stack(preds).mean(0)\n",
    "\n",
    "predictions = torch.stack([predict_mask(fname, checkpoints) for fname in tqdm(fnames)])\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "\n",
    "images = [torch.tensor(load_image(fname)) for fname in fnames[:N]]\n",
    "masks = [torch.zeros_like(image) for image in predictions[:N]]\n",
    "weights = [torch.zeros_like(image) for image in predictions[:N]]\n",
    "\n",
    "images = torch.stack(images)\n",
    "masks = torch.stack(masks)\n",
    "weights = torch.stack(weights)\n",
    "\n",
    "plot_predictions(\n",
    "    images=images,\n",
    "    masks=masks,\n",
    "    predictions=predictions[:N],\n",
    "    # weights=predictions[:N] > .5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = \"../data/preds/\"\n",
    "\n",
    "for fname, prediction in tqdm(zip(fnames, predictions), total=len(fnames), ncols=80):\n",
    "    out_fname = os.path.join(pred_path, os.path.basename(fname))\n",
    "    prediction = prediction.numpy() > 0.5\n",
    "    prediction = prediction.astype(np.uint8) * 255\n",
    "    prediction = np.stack([prediction, prediction, prediction], axis=-1)\n",
    "    save_mask(prediction, out_fname)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = os.listdir(pred_path)\n",
    "fnames = [os.path.join(pred_path, fname) for fname in fnames if fname.endswith(\".png\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_to_submission(\n",
    "    \"../data/submission.csv\",\n",
    "    \"\",\n",
    "    *sorted(fnames),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.315142198308992"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_score = 0.91513\n",
    "\n",
    "max_score = 0.94186\n",
    "baseline_score = 0.86380\n",
    "\n",
    "4 + 2 * (our_score - baseline_score) / (max_score - baseline_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
